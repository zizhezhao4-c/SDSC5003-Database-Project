import time
import json
import logging
import random
import threading
from datetime import timedelta, datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests

# è‡ªå®šä¹‰åº“å¯¼å…¥
import fetch_all_alphas as faa
import wq_login
from session_manager import SessionManager
from session_proxy import SessionProxy

# é…ç½®æ—¥å¿—è®°å½•
# ç»Ÿä¸€æ—¥å¿—æ—¶é—´ä¸ºUTCï¼Œå¹¶åœ¨æ—¶é—´åæ·»åŠ Zæ ‡è®°
try:
    from wq_logger import get_logger as _get_logger
    logger = _get_logger(__name__)
    # ä¿æŒå­—æ®µä¸æ ¼å¼ä¸€è‡´ï¼šUTC+Zï¼ŒthreadNameã€levelnameã€message
    _formatter = logging.Formatter('%(asctime)s - %(threadName)s - %(levelname)s - %(message)s', '%Y-%m-%d %H:%M:%SZ')
    for _h in logging.getLogger().handlers:
        _h.setFormatter(_formatter)
except Exception:
    logging.Formatter.converter = time.gmtime
    if not logging.getLogger().handlers:
        logging.basicConfig(level=logging.INFO, format='%(asctime)sZ - %(threadName)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)

# --------------------------------------------------------------------------------------
# è¿è¡Œé…ç½®åŠ è½½ï¼ˆä¼˜å…ˆ async_config.jsonï¼Œä¸å­˜åœ¨æ—¶ç”¨å†…ç½®é»˜è®¤ï¼‰
# --------------------------------------------------------------------------------------
CONFIG_FILE = Path(__file__).with_name("async_config.json")
DEFAULT_CONFIG = {
    "MAX_WORKERS": 3,
    "MAX_RETRIES": 3,
    "INITIAL_BACKOFF": 8,
    "BACKOFF_MULTIPLIER": 2.0,
    "MAX_BACKOFF": 240,
    "MAX_LOGIN_ATTEMPTS": 5,
    "SESSION_CHECK_INTERVAL": 40,
    "SESSION_CHECK_ON_RETRY": 6,     # é‡è¯•Næ¬¡åæ£€æŸ¥ä¼šè¯
    "SESSION_CHECK_ON_CONSECUTIVE_FAILURES": 3,  # è¿ç»­å¤±è´¥Næ¬¡åæ£€æŸ¥ä¼šè¯
    "FETCH_MODE": "auto",            # è·å–æ¨¡å¼ï¼šauto, incremental, manual
    "START_TIME": None,              # manual æ¨¡å¼çš„èµ·å§‹æ—¶é—´
    "END_TIME": None,                # manual æ¨¡å¼çš„ç»“æŸæ—¶é—´
    "INTERVAL_HOURS": 1,
    "DAYS": 14,
    "OVERWRITE": True,
    "SKIP_REGULAR": False,
    "SKIP_SUPER": False,
    "BASE_DIRECTORY": None,          # å¯é€‰ï¼šè¦†ç›–æ•°æ®è¾“å‡ºæ ¹ç›®å½•ï¼Œç¤ºä¾‹ "./WorldQuant/Data/alphas"
    "RETRY_ROUNDS": 3,               # å¤±è´¥åˆ‡ç‰‡é‡è¯•è½®æ•°
    "RETRY_WORKERS": 2               # é‡è¯•æ—¶ä½¿ç”¨çš„å¹¶å‘workeræ•°é‡ï¼ˆé€šå¸¸æ¯”åˆæ¬¡è·å–å°‘ï¼‰
}

def _load_runtime_config():
    cfg = DEFAULT_CONFIG.copy()
    try:
        if CONFIG_FILE.exists():
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                file_cfg = json.load(f)
            if isinstance(file_cfg, dict):
                # æ”¯æŒå¤§å°å†™ä¸æ•æ„Ÿåˆå¹¶ï¼Œä»¥é…ç½®æ–‡ä»¶ä¸ºå‡†è¦†ç›–
                merged = cfg.copy()
                for k, v in file_cfg.items():
                    # è¿‡æ»¤æ‰æ³¨é‡Šå­—æ®µï¼ˆä»¥_æˆ–//å¼€å¤´çš„é”®ï¼‰
                    if not k.startswith('_') and not k.startswith('//'):
                        merged[k.upper()] = v
                cfg = merged
            logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {CONFIG_FILE}")
        else:
            logger.info("æœªå‘ç° async_config.jsonï¼Œä½¿ç”¨å†…ç½®é»˜è®¤é…ç½®")
    except Exception as e:
        logger.warning(f"åŠ è½½ async_config.json å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®é»˜è®¤é…ç½®: {e}")
    return cfg

_CONFIG = _load_runtime_config()

# --------------------------------------------------------------------------------------
# å¼‚æ­¥é…ç½®å‚æ•°ï¼ˆç”± _CONFIG èµ‹å€¼ï¼Œä¾›å…¨å±€é‡è¯•/é€€é¿é€»è¾‘ä½¿ç”¨ï¼‰
# --------------------------------------------------------------------------------------
MAX_WORKERS = int(_CONFIG["MAX_WORKERS"])
MAX_RETRIES = int(_CONFIG["MAX_RETRIES"])
INITIAL_BACKOFF = int(_CONFIG["INITIAL_BACKOFF"])
BACKOFF_MULTIPLIER = float(_CONFIG["BACKOFF_MULTIPLIER"])
MAX_BACKOFF = int(_CONFIG["MAX_BACKOFF"])
MAX_LOGIN_ATTEMPTS = int(_CONFIG["MAX_LOGIN_ATTEMPTS"])  # ç»Ÿä¸€å¤–å±‚ç™»å½•è§¦å‘ä¸Šé™

# ä¼šè¯éªŒè¯è®¡æ•°å™¨
_SUCCESS_COUNT = 0
_SESSION_CHECK_INTERVAL = int(_CONFIG["SESSION_CHECK_INTERVAL"])  # æ¯ N æ¬¡æˆåŠŸåæ£€æŸ¥ä¼šè¯
_SESSION_CHECK_ON_RETRY = int(_CONFIG.get("SESSION_CHECK_ON_RETRY", 6))  # é‡è¯•Næ¬¡åæ£€æŸ¥ä¼šè¯
_SESSION_CHECK_ON_CONSECUTIVE_FAILURES = int(_CONFIG.get("SESSION_CHECK_ON_CONSECUTIVE_FAILURES", 3))  # è¿ç»­å¤±è´¥Næ¬¡åæ£€æŸ¥ä¼šè¯
_GLOBAL_LOCK = threading.Lock()

# ç®€åŒ–ï¼šç§»é™¤åˆ†ä½œç”¨åŸŸåˆ·æ–°è®¡æ•°ï¼Œä»…ä¿ç•™ç»Ÿä¸€çš„ä¼šè¯ç®¡ç†ä¸æ—¥å¿—

# --------------------------------------------------------------------------------------
# å¼‚æ­¥ä¼šè¯ç®¡ç†
# --------------------------------------------------------------------------------------
# æ³¨æ„ï¼šä½¿ç”¨SessionProxyåï¼Œä¸å†éœ€è¦_get_shared_sessionå‡½æ•°
# SessionProxyä¼šè‡ªåŠ¨åœ¨æ¯æ¬¡è¯·æ±‚æ—¶è·å–æœ€æ–°sessionï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨

def _increment_success_count():
    """å¢åŠ æˆåŠŸè®¡æ•°å™¨"""
    global _SUCCESS_COUNT
    with _GLOBAL_LOCK:
        _SUCCESS_COUNT += 1

# --------------------------------------------------------------------------------------
# å¼‚æ­¥æ•°æ®è·å–æ ¸å¿ƒå‡½æ•°
# --------------------------------------------------------------------------------------

def check_file_exists(userkey, base_directory, slice_start, slice_end, alpha_type="regular"):
    """æ£€æŸ¥æŒ‡å®šæ—¶é—´åˆ‡ç‰‡çš„æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨"""
    import re
    safe_start_date = re.sub(r'[:T]', '-', slice_start)
    safe_end_date = re.sub(r'[:T]', '-', slice_end)
    filename = f'{userkey}_{safe_start_date}_to_{safe_end_date}_all_{alpha_type}_alphas.json'
    directory = base_directory / f"all_{alpha_type}_alphas"
    file_path = directory / filename
    return file_path.exists(), file_path

def fetch_single_time_slice(session_proxy: SessionProxy, session_manager: SessionManager, 
                          userkey: str, base_directory: Path,
                          slice_start: str, slice_end: str, alpha_type: str = "regular",
                          overwrite: bool = False, worker_id: int = 0) -> Tuple[bool, str, Optional[str]]:
    """
    è·å–å•ä¸ªæ—¶é—´åˆ‡ç‰‡çš„æ•°æ® - ä½¿ç”¨SessionProxyç¡®ä¿å§‹ç»ˆä½¿ç”¨æœ€æ–°session
    
    Args:
        session_proxy: SessionProxyå®ä¾‹ï¼Œè‡ªåŠ¨è·å–æœ€æ–°session
        session_manager: SessionManagerå®ä¾‹ï¼Œç”¨äºåœ¨401é”™è¯¯æ—¶å¼ºåˆ¶åˆ·æ–°
        userkey: ç”¨æˆ·æ ‡è¯†
        base_directory: åŸºç¡€ç›®å½•
        slice_start: åˆ‡ç‰‡èµ·å§‹æ—¶é—´
        slice_end: åˆ‡ç‰‡ç»“æŸæ—¶é—´
        alpha_type: alphaç±»å‹
        overwrite: æ˜¯å¦è¦†ç›–
        worker_id: Worker ID
    
    Returns:
        (success, message, filename)
    """
    thread_name = f"Worker-{worker_id}"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    file_exists, file_path = check_file_exists(userkey, base_directory, slice_start, slice_end, alpha_type)
    
    if file_exists and not overwrite:
        return True, f"[{thread_name}] æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {file_path.name}", file_path.name
    
    if file_exists and overwrite:
        logger.info(f"[{thread_name}] æ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¦†ç›–: {file_path.name}")
    
    # çº¿ç¨‹é—´éšæœºé”™å¼€æ—¶é—´ï¼Œé¿å…åŒæ—¶è¯·æ±‚
    random_delay = random.uniform(0.5, 2.0)  # 0.5-2ç§’éšæœºå»¶è¿Ÿ
    time.sleep(random_delay)
    
    retries = 0
    consecutive_failures = 0  # è¿ç»­å¤±è´¥è®¡æ•°å™¨
    
    while retries < MAX_RETRIES:
        try:
            # ä½¿ç”¨SessionProxyï¼Œæ¯æ¬¡è¯·æ±‚è‡ªåŠ¨è·å–æœ€æ–°session
            # ä¸å†éœ€è¦æ‰‹åŠ¨è·å–å’Œæ›´æ–°session
            
            # æ ¹æ®ç±»å‹è°ƒç”¨ç›¸åº”çš„å‡½æ•°
            # æ³¨æ„ï¼šä¸å†è¿”å›updated_sessionï¼ŒSessionProxyä¼šè‡ªåŠ¨ç®¡ç†session
            if alpha_type == "regular":
                fetched_alphas, filename, info = faa.fetch_and_save_regular_alphas(
                    session_proxy, slice_start, slice_end, userkey, base_directory)
            else:
                fetched_alphas, filename, info = faa.fetch_and_save_super_alphas(
                    session_proxy, slice_start, slice_end, userkey, base_directory)
            
            if fetched_alphas:
                # å¦‚æœä¸å…¨ï¼Œæ ‡è®°ä¸ºå¤±è´¥ä»¥è¿›å…¥é‡è¯•ï¼Œä½†ä»å·²ä¿å­˜å½“å‰æ•°æ®
                if info and info.get('incomplete'):
                    missing = info.get('skipped_offsets', [])
                    return False, f"[Scope:Slice][{thread_name}] éƒ¨åˆ†æˆåŠŸï¼Œç¼ºå¤±é¡µ offsets: {missing} -> {filename}", filename
                # å®Œæ•´æˆåŠŸ - é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°å™¨
                consecutive_failures = 0
                _increment_success_count()
                return True, f"[Scope:Slice][{thread_name}] æˆåŠŸè·å– {len(fetched_alphas)} ä¸ª{alpha_type} alphas -> {filename}", filename
            else:
                # è¿”å›Noneè¡¨ç¤ºå¤±è´¥ï¼Œå¢åŠ å¤±è´¥è®¡æ•°
                consecutive_failures += 1
                # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯ä¼šè¯
                # SessionProxyç¡®ä¿æ¯æ¬¡è¯·æ±‚éƒ½ä½¿ç”¨æœ€æ–°sessionï¼Œä½†è¿ç»­å¤±è´¥å¯èƒ½è¡¨ç¤ºsessioné—®é¢˜
                if consecutive_failures >= _SESSION_CHECK_ON_CONSECUTIVE_FAILURES:
                    logger.warning(f"[Scope:Slice][{thread_name}] WARNING: è¿ç»­å¤±è´¥{consecutive_failures}æ¬¡ï¼Œä¸»åŠ¨æ£€æŸ¥ä¼šè¯æœ‰æ•ˆæ€§")
                    try:
                        import wq_login
                        # ä¿®å¤ï¼šåœ¨è°ƒç”¨check_session_validityä¹‹å‰æ£€æŸ¥sessionæ˜¯å¦ä¸ºNone
                        if session_manager.session is None:
                            logger.warning(f"[Scope:Slice][{thread_name}] ä¼šè¯ä¸ºNoneï¼Œä¸»åŠ¨åˆ·æ–°")
                            with _GLOBAL_LOCK:
                                session_manager.force_refresh()
                                logger.info(f"[Scope:Slice][{thread_name}] SUCCESS: ä¼šè¯å·²åˆ·æ–°ï¼ŒSessionProxyå°†åœ¨ä¸‹æ¬¡è¯·æ±‚æ—¶è‡ªåŠ¨ä½¿ç”¨æ–°session")
                        elif not wq_login.check_session_validity(session_manager.session, min_remaining_seconds=1800):
                            logger.warning(f"[Scope:Slice][{thread_name}] ä¼šè¯å‰©ä½™æ—¶é—´ä¸è¶³30åˆ†é’Ÿï¼Œä¸»åŠ¨åˆ·æ–°")
                            with _GLOBAL_LOCK:
                                session_manager.force_refresh()
                                logger.info(f"[Scope:Slice][{thread_name}] SUCCESS: ä¼šè¯å·²åˆ·æ–°ï¼ŒSessionProxyå°†åœ¨ä¸‹æ¬¡è¯·æ±‚æ—¶è‡ªåŠ¨ä½¿ç”¨æ–°session")
                        else:
                            logger.info(f"[Scope:Slice][{thread_name}] SUCCESS: ä¼šè¯éªŒè¯é€šè¿‡ï¼Œä»ç„¶æœ‰æ•ˆ")
                    except Exception as check_error:
                        logger.error(f"[Scope:Slice][{thread_name}] ERROR: æ£€æŸ¥ä¼šè¯æ—¶å‡ºé”™: {check_error}")
                
                return False, f"[Scope:Slice][{thread_name}] æœªè·å–åˆ°æ•°æ®: {slice_start} åˆ° {slice_end}", None
                
        except Exception as e:
            retries += 1
            consecutive_failures += 1
            error_msg = str(e)
            
            # æ£€æŸ¥é”™è¯¯ç±»å‹å¹¶é‡‡å–ç›¸åº”æªæ–½
            if "401" in error_msg or "403" in error_msg or "unauthorized" in error_msg.lower():
                # è®¤è¯é”™è¯¯ï¼šä¼šè¯è¿‡æœŸï¼Œå¼ºåˆ¶åˆ·æ–°
                # SessionProxyç¡®ä¿ä¸‹æ¬¡è¯·æ±‚è‡ªåŠ¨ä½¿ç”¨æ–°sessionï¼Œæ— éœ€æ‰‹åŠ¨æ›´æ–°
                logger.warning(f"[Scope:Slice][{thread_name}] æ”¶åˆ°401/403è®¤è¯é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°ä¼šè¯: {e}")
                try:
                    with _GLOBAL_LOCK:
                        session_manager.force_refresh()
                        logger.info(f"[Scope:Slice][{thread_name}] SUCCESS: ä¼šè¯å·²åˆ·æ–°ï¼ŒSessionProxyå°†åœ¨ä¸‹æ¬¡è¯·æ±‚æ—¶è‡ªåŠ¨ä½¿ç”¨æ–°session")
                    # åˆ·æ–°æˆåŠŸåï¼Œä¸ç­‰å¾…ç›´æ¥é‡è¯•ï¼ˆSessionProxyä¼šè·å–æ–°sessionï¼‰
                    continue
                except Exception as login_error:
                    logger.error(f"[Scope:Slice][{thread_name}] ERROR: åˆ·æ–°ä¼šè¯å¤±è´¥: {login_error}")
                    break
                    
            elif retries >= _SESSION_CHECK_ON_RETRY:
                # é‡è¯•Næ¬¡åï¼Œæ£€æŸ¥ä¼šè¯æœ‰æ•ˆæ€§
                # SessionProxyç¡®ä¿æ¯æ¬¡è¯·æ±‚éƒ½ä½¿ç”¨æœ€æ–°sessionï¼Œä½†å¤šæ¬¡é‡è¯•å¯èƒ½è¡¨ç¤ºsessioné—®é¢˜
                logger.warning(f"[Scope:Slice][{thread_name}] WARNING: å·²é‡è¯•{retries}æ¬¡ï¼Œä¸»åŠ¨æ£€æŸ¥ä¼šè¯æœ‰æ•ˆæ€§")
                try:
                    import wq_login
                    # ä¿®å¤ï¼šåœ¨è°ƒç”¨check_session_validityä¹‹å‰æ£€æŸ¥sessionæ˜¯å¦ä¸ºNone
                    if session_manager.session is None:
                        logger.warning(f"[Scope:Slice][{thread_name}] ä¼šè¯ä¸ºNoneï¼Œä¸»åŠ¨åˆ·æ–°")
                        with _GLOBAL_LOCK:
                            session_manager.force_refresh()
                            logger.info(f"[Scope:Slice][{thread_name}] SUCCESS: ä¼šè¯å·²åˆ·æ–°ï¼ŒSessionProxyå°†åœ¨ä¸‹æ¬¡è¯·æ±‚æ—¶è‡ªåŠ¨ä½¿ç”¨æ–°session")
                    elif not wq_login.check_session_validity(session_manager.session, min_remaining_seconds=1800):
                        logger.warning(f"[Scope:Slice][{thread_name}] ä¼šè¯å‰©ä½™æ—¶é—´ä¸è¶³30åˆ†é’Ÿï¼Œä¸»åŠ¨åˆ·æ–°")
                        with _GLOBAL_LOCK:
                            session_manager.force_refresh()
                            logger.info(f"[Scope:Slice][{thread_name}] SUCCESS: ä¼šè¯å·²åˆ·æ–°ï¼ŒSessionProxyå°†åœ¨ä¸‹æ¬¡è¯·æ±‚æ—¶è‡ªåŠ¨ä½¿ç”¨æ–°session")
                    else:
                        logger.info(f"[Scope:Slice][{thread_name}] SUCCESS: ä¼šè¯éªŒè¯é€šè¿‡ï¼Œç»§ç»­é‡è¯•")
                except Exception as check_error:
                    logger.error(f"[Scope:Slice][{thread_name}] ERROR: æ£€æŸ¥ä¼šè¯æ—¶å‡ºé”™: {check_error}")
                    
            # ç­‰å¾…åé‡è¯•
            if retries < MAX_RETRIES:
                if "429" in error_msg or "rate limit" in error_msg.lower():
                    # é™æµé”™è¯¯ï¼šæŒ‡æ•°é€€é¿
                    base_wait = INITIAL_BACKOFF * (BACKOFF_MULTIPLIER ** (retries - 1))
                    wait_time = min(MAX_BACKOFF, base_wait)
                    wait_time *= random.uniform(0.8, 1.2)
                    logger.warning(f"[Scope:Slice][{thread_name}] 429 é™æµï¼Œç­‰å¾… {wait_time:.1f}s (é‡è¯• {retries}/{MAX_RETRIES})")
                else:
                    # å…¶ä»–é”™è¯¯ï¼šçº¿æ€§é€€é¿
                    wait_time = min(30, 5 * retries)
                    wait_time *= random.uniform(0.8, 1.2)
                    logger.warning(f"[Scope:Slice][{thread_name}] è·å–æ•°æ®å¤±è´¥: {e}ï¼Œç­‰å¾… {wait_time:.1f}s åé‡è¯• ({retries}/{MAX_RETRIES})")
                time.sleep(wait_time)
            else:
                logger.error(f"[Scope:Slice][{thread_name}] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒ: {e}")
    
    return False, f"[Scope:Slice][{thread_name}] å¤±è´¥: {slice_start} åˆ° {slice_end} (é‡è¯• {MAX_RETRIES} æ¬¡åæ”¾å¼ƒ)", None

# --------------------------------------------------------------------------------------
# å¼‚æ­¥æ‰¹å¤„ç†å‡½æ•°
# --------------------------------------------------------------------------------------

def fetch_alpha_async(session_proxy: SessionProxy, session_manager: SessionManager,
                     userkey: str, start_date_input: str, 
                     end_date_input: str, base_directory: Path, alpha_type: str = "regular", 
                     overwrite: bool = False, max_workers: int = MAX_WORKERS, interval_hours: int = 2) -> Dict[str, Any]:
    """
    å¼‚æ­¥è·å–alphaæ•°æ®
    
    Args:
        session_proxy: SessionProxyå®ä¾‹ï¼Œè‡ªåŠ¨è·å–æœ€æ–°session
        session_manager: SessionManagerå®ä¾‹ï¼Œç”¨äºåœ¨401é”™è¯¯æ—¶å¼ºåˆ¶åˆ·æ–°
        userkey: ç”¨æˆ·æ ‡è¯†
        start_date_input: èµ·å§‹æ—¶é—´
        end_date_input: ç»“æŸæ—¶é—´
        base_directory: åŸºç¡€ç›®å½•
        alpha_type: alphaç±»å‹
        overwrite: æ˜¯å¦è¦†ç›–
        max_workers: å¹¶å‘workeræ•°é‡
        interval_hours: æ—¶é—´åˆ‡ç‰‡é—´éš”ï¼ˆå°æ—¶ï¼‰
    
    Returns:
        ç»Ÿè®¡ç»“æœå­—å…¸
    """
    # ç”Ÿæˆæ—¶é—´åˆ‡ç‰‡
    time_slices = list(faa.generate_time_slices(start_date_input, end_date_input, interval=timedelta(hours=interval_hours)))
    
    logger.info(f"å¼€å§‹å¼‚æ­¥è·å– {alpha_type.upper()} Alphasï¼Œå…± {len(time_slices)} ä¸ªæ—¶é—´åˆ‡ç‰‡ï¼Œä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘worker")
    
    # åˆ›å»ºç»“æœè·Ÿè¸ª
    results = {
        'total': len(time_slices),
        'processed': 0,
        'skipped': 0,
        'failed': 0,
        'incomplete': 0,
        'failed_slices': [],
        'incomplete_slices': [],
        'retry_queue': [],            # éœ€è¦è¿›å…¥å¤šè½®é‡è¯•çš„åˆ‡ç‰‡ï¼ˆåŒ…å«å¤±è´¥ä¸ä¸å…¨ï¼‰
        'retry_types': {},            # slice_info -> 'failed' | 'incomplete'
        'successful_files': [],
        'failed_reasons': {}
    }
    
    failed_lock = threading.Lock()

    def mark_failed(slice_info: str, error_msg: str = ""):
        with failed_lock:
            results['failed_slices'].append(slice_info)
            results['failed_reasons'][slice_info] = error_msg
            results['retry_queue'].append(slice_info)
            results['retry_types'][slice_info] = 'failed'
            # ç»Ÿä¸€é€šè¿‡loggerè®°å½•å¤±è´¥ä¿¡æ¯
            logger.error(f"[ç¬¬ä¸€è½®è·å–] åˆ‡ç‰‡å¤±è´¥: {slice_info} - {error_msg}")

    def mark_incomplete(slice_info: str, note: str = ""):
        with failed_lock:
            results['incomplete_slices'].append(slice_info)
            results['retry_queue'].append(slice_info)
            results['retry_types'][slice_info] = 'incomplete'
            # ç»Ÿä¸€é€šè¿‡loggerè®°å½•ä¸å…¨ä¿¡æ¯
            logger.warning(f"[ç¬¬ä¸€è½®è·å–] åˆ‡ç‰‡ä¸å…¨: {slice_info} - {note}")
    
    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¼‚æ­¥è·å–
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix=f"{alpha_type}Worker") as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡ - æ‰€æœ‰workerå…±äº«åŒä¸€ä¸ªsession_proxy
        future_to_slice = {}
        for i, (slice_start, slice_end) in enumerate(time_slices):
            future = executor.submit(
                fetch_single_time_slice,
                session_proxy, session_manager, userkey, base_directory,  # ä¼ é€’session_proxyå’Œsession_manager
                slice_start, slice_end, alpha_type, overwrite, i + 1
            )
            future_to_slice[future] = (i, slice_start, slice_end)
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        completed = 0
        for future in as_completed(future_to_slice):
            slice_index, slice_start, slice_end = future_to_slice[future]
            completed += 1
            
            try:
                success, message, filename = future.result()
                
                if success:
                    if "è·³è¿‡" in message:
                        results['skipped'] += 1
                    else:
                        results['processed'] += 1
                        if filename:
                            results['successful_files'].append(filename)
                    logger.info(f"[{completed}/{len(time_slices)}] {message}")
                else:
                    slice_info = f"{slice_start}_to_{slice_end}"
                    # åŒºåˆ†ä¸å…¨ vs å¤±è´¥
                    if "éƒ¨åˆ†æˆåŠŸ" in message or "ä¸å…¨" in message:
                        results['incomplete'] += 1
                        mark_incomplete(slice_info, message)
                        logger.warning(f"[{completed}/{len(time_slices)}] {message}")
                    else:
                        results['failed'] += 1
                        mark_failed(slice_info, message.split('] ')[-1] if '] ' in message else message)
                        logger.error(f"[{completed}/{len(time_slices)}] {message}")
                    
            except Exception as e:
                results['failed'] += 1
                slice_info = f"{slice_start}_to_{slice_end}"
                mark_failed(slice_info, f"Workerå¼‚å¸¸: {str(e)}")
                logger.error(f"[{completed}/{len(time_slices)}] Workerå¼‚å¸¸: {slice_start} åˆ° {slice_end}: {e}")
            
            # æ¯10ä¸ªä»»åŠ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if completed % 10 == 0:
                logger.info(f"è¿›åº¦: {completed}/{len(time_slices)} å®Œæˆï¼ŒæˆåŠŸè®¡æ•°: {_SUCCESS_COUNT}")
    
    # ç¬¬ä¸€è½®è·å–å®Œæˆï¼Œå¼€å§‹é‡è¯•å¤±è´¥çš„åˆ‡ç‰‡
    retry_rounds = int(_CONFIG.get("RETRY_ROUNDS", 3))
    retry_workers = int(_CONFIG.get("RETRY_WORKERS", 2))
    
    # åˆå¹¶å¤±è´¥å’Œä¸å…¨çš„åˆ‡ç‰‡è¿›è¡Œé‡è¯•
    all_failed_slices = results['failed_slices'] + results['incomplete_slices']
    
    if all_failed_slices and retry_rounds > 0:
        logger.info(f"\nğŸ“‹ ç¬¬ä¸€è½®è·å–å®Œæˆï¼Œå¼€å§‹é‡è¯• {len(all_failed_slices)} ä¸ªå¤±è´¥/ä¸å…¨åˆ‡ç‰‡")
        logger.info(f"é‡è¯•é…ç½®ï¼š{retry_rounds} è½®é‡è¯•ï¼Œæ¯è½®ä½¿ç”¨ {retry_workers} ä¸ªworker")
        
        current_failed_slices = all_failed_slices.copy()
        
        for retry_round in range(1, retry_rounds + 1):
            if not current_failed_slices:
                logger.info(f"SUCCESS: æ‰€æœ‰åˆ‡ç‰‡å·²æˆåŠŸï¼Œæ— éœ€ç»§ç»­é‡è¯•")
                break
                
            # æ‰§è¡Œé‡è¯•
            retry_results = retry_failed_slices(
                session_proxy, session_manager, userkey, base_directory,
                current_failed_slices, alpha_type, overwrite, retry_workers, retry_round
            )
            
            # æ›´æ–°æ€»ä½“ç»Ÿè®¡
            results['processed'] += retry_results['processed']
            results['skipped'] += retry_results['skipped']
            results['successful_files'].extend(retry_results['successful_files'])
            
            # è¾“å‡ºé‡è¯•è½®æ¬¡ç»“æœ
            logger.info(f"\nç¬¬ {retry_round} è½®é‡è¯•ç»“æœ:")
            logger.info(f"  æˆåŠŸ: {retry_results['processed']} ä¸ª")
            logger.info(f"  è·³è¿‡: {retry_results['skipped']} ä¸ª") 
            logger.info(f"  å¤±è´¥: {retry_results['failed']} ä¸ª")
            logger.info(f"  ä¸å…¨: {retry_results['incomplete']} ä¸ª")
            
            # å‡†å¤‡ä¸‹ä¸€è½®é‡è¯•çš„åˆ‡ç‰‡ï¼ˆå¤±è´¥+ä¸å…¨ï¼‰
            current_failed_slices = retry_results['failed_slices'] + retry_results['incomplete_slices']
            
            if not current_failed_slices:
                logger.info(f"SUCCESS: ç¬¬ {retry_round} è½®é‡è¯•åæ‰€æœ‰åˆ‡ç‰‡å·²æˆåŠŸ")
                break
            elif retry_round < retry_rounds:
                logger.info(f"å‡†å¤‡ç¬¬ {retry_round + 1} è½®é‡è¯•ï¼Œå‰©ä½™ {len(current_failed_slices)} ä¸ªåˆ‡ç‰‡")
                # é‡è¯•é—´éš”ç­‰å¾…
                time.sleep(5)
        
        # æ›´æ–°æœ€ç»ˆçš„å¤±è´¥ç»Ÿè®¡
        if current_failed_slices:
            # é‡æ–°åˆ†ç±»æœ€ç»ˆå¤±è´¥çš„åˆ‡ç‰‡
            final_failed = []
            final_incomplete = []
            for slice_info in current_failed_slices:
                if slice_info in results['failed_slices']:
                    final_failed.append(slice_info)
                else:
                    final_incomplete.append(slice_info)
            
            results['failed_slices'] = final_failed
            results['incomplete_slices'] = final_incomplete
            results['failed'] = len(final_failed)
            results['incomplete'] = len(final_incomplete)
            
            logger.info(f"\nERROR: ç»è¿‡ {retry_rounds} è½®é‡è¯•åä»ç„¶å¤±è´¥çš„åˆ‡ç‰‡:")
            for s in final_failed:
                reason = results.get('failed_reasons', {}).get(s, 'æœªçŸ¥åŸå› ')
                logger.info(f"- {s}: {reason}")
            
            if final_incomplete:
                logger.info(f"\nWARNING: ç»è¿‡ {retry_rounds} è½®é‡è¯•åä»ç„¶ä¸å…¨çš„åˆ‡ç‰‡:")
                for s in final_incomplete:
                    logger.info(f"- {s}")
        else:
            # æ‰€æœ‰åˆ‡ç‰‡éƒ½æˆåŠŸäº†
            results['failed_slices'] = []
            results['incomplete_slices'] = []
            results['failed'] = 0
            results['incomplete'] = 0
            logger.info(f"ğŸ‰ æ‰€æœ‰åˆ‡ç‰‡ç»è¿‡é‡è¯•åå‡å·²æˆåŠŸè·å–ï¼")
    
    elif all_failed_slices:
        logger.info(f"\nğŸ“‹ æœ‰ {len(all_failed_slices)} ä¸ªå¤±è´¥/ä¸å…¨åˆ‡ç‰‡ï¼Œä½†é‡è¯•åŠŸèƒ½å·²ç¦ç”¨ (RETRY_ROUNDS=0)")
        # è¾“å‡ºæœ€ç»ˆå¤±è´¥åŸå› åˆ—è¡¨
        if results['failed_slices']:
            logger.info("æœ€ç»ˆå¤±è´¥åˆ‡ç‰‡ä¸åŸå› ï¼š")
            for s in results['failed_slices']:
                reason = results.get('failed_reasons', {}).get(s, 'æœªçŸ¥åŸå› ')
                logger.info(f"- {s}: {reason}")

        if results['incomplete_slices']:
            logger.info("æœ€ç»ˆä¸å…¨åˆ‡ç‰‡ï¼š")
            for s in results['incomplete_slices']:
                logger.info(f"- {s}")
    else:
        logger.info(f"ğŸ‰ ç¬¬ä¸€è½®è·å–å³å…¨éƒ¨æˆåŠŸï¼Œæ— éœ€é‡è¯•ï¼")

    return results

def retry_failed_slices(session_proxy: SessionProxy, session_manager: SessionManager,
                       userkey: str, base_directory: Path,
                       failed_slices: List[str], alpha_type: str = "regular", 
                       overwrite: bool = False, max_workers: int = 2, retry_round: int = 1) -> Dict[str, Any]:
    """
    é‡è¯•å¤±è´¥çš„æ—¶é—´åˆ‡ç‰‡
    
    Args:
        session_proxy: SessionProxyå®ä¾‹ï¼Œè‡ªåŠ¨è·å–æœ€æ–°session
        session_manager: SessionManagerå®ä¾‹ï¼Œç”¨äºåœ¨401é”™è¯¯æ—¶å¼ºåˆ¶åˆ·æ–°
        userkey: ç”¨æˆ·æ ‡è¯†
        base_directory: åŸºç¡€ç›®å½•
        failed_slices: å¤±è´¥çš„åˆ‡ç‰‡åˆ—è¡¨ï¼Œæ ¼å¼ä¸º "start_to_end"
        alpha_type: alphaç±»å‹
        overwrite: æ˜¯å¦è¦†ç›–
        max_workers: å¹¶å‘workeræ•°é‡
        retry_round: é‡è¯•è½®æ¬¡
    
    Returns:
        é‡è¯•ç»“æœç»Ÿè®¡
    """
    if not failed_slices:
        return {
            'total': 0, 'processed': 0, 'skipped': 0, 'failed': 0, 'incomplete': 0,
            'failed_slices': [], 'incomplete_slices': [], 'successful_files': [], 'failed_reasons': {}
        }
    
    logger.info(f"\nRetry round {retry_round}: {len(failed_slices)} failed slices, using {max_workers} concurrent workers")
    
    # è§£æå¤±è´¥åˆ‡ç‰‡çš„æ—¶é—´èŒƒå›´
    time_slices = []
    for slice_info in failed_slices:
        try:
            start_str, end_str = slice_info.split('_to_')
            time_slices.append((start_str, end_str))
        except ValueError:
            logger.warning(f"æ— æ³•è§£æåˆ‡ç‰‡æ ¼å¼: {slice_info}")
            continue
    
    if not time_slices:
        logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„åˆ‡ç‰‡å¯ä»¥é‡è¯•")
        return {
            'total': 0, 'processed': 0, 'skipped': 0, 'failed': 0, 'incomplete': 0,
            'failed_slices': [], 'incomplete_slices': [], 'successful_files': [], 'failed_reasons': {}
        }
    
    # åˆ›å»ºé‡è¯•ç»“æœè·Ÿè¸ª
    retry_results = {
        'total': len(time_slices),
        'processed': 0,
        'skipped': 0,
        'failed': 0,
        'incomplete': 0,
        'failed_slices': [],
        'incomplete_slices': [],
        'successful_files': [],
        'failed_reasons': {}
    }
    
    retry_failed_lock = threading.Lock()

    def mark_retry_failed(slice_info: str, error_msg: str = ""):
        with retry_failed_lock:
            retry_results['failed_slices'].append(slice_info)
            retry_results['failed_reasons'][slice_info] = error_msg
            # ç»Ÿä¸€é€šè¿‡loggerè®°å½•ï¼Œä¸å•ç‹¬åˆ›å»ºæ–‡ä»¶
            logger.error(f"[é‡è¯•ç¬¬{retry_round}è½®] åˆ‡ç‰‡å¤±è´¥: {slice_info} - {error_msg}")

    def mark_retry_incomplete(slice_info: str, note: str = ""):
        with retry_failed_lock:
            retry_results['incomplete_slices'].append(slice_info)
            # ç»Ÿä¸€é€šè¿‡loggerè®°å½•
            logger.warning(f"[é‡è¯•ç¬¬{retry_round}è½®] åˆ‡ç‰‡ä¸å…¨: {slice_info} - {note}")
    
    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œé‡è¯•
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix=f"Retry{alpha_type}Worker") as executor:
        # æäº¤æ‰€æœ‰é‡è¯•ä»»åŠ¡
        future_to_slice = {}
        for i, (slice_start, slice_end) in enumerate(time_slices):
            future = executor.submit(
                fetch_single_time_slice,
                session_proxy, session_manager, userkey, base_directory,
                slice_start, slice_end, alpha_type, overwrite, i + 1
            )
            future_to_slice[future] = (i, slice_start, slice_end)
        
        # å¤„ç†å®Œæˆçš„é‡è¯•ä»»åŠ¡
        completed = 0
        for future in as_completed(future_to_slice):
            slice_index, slice_start, slice_end = future_to_slice[future]
            completed += 1
            
            try:
                success, message, filename = future.result()
                
                if success:
                    if "è·³è¿‡" in message:
                        retry_results['skipped'] += 1
                    else:
                        retry_results['processed'] += 1
                        if filename:
                            retry_results['successful_files'].append(filename)
                    logger.info(f"[é‡è¯• {completed}/{len(time_slices)}] SUCCESS: {message}")
                else:
                    slice_info = f"{slice_start}_to_{slice_end}"
                    # åŒºåˆ†ä¸å…¨ vs å¤±è´¥
                    if "éƒ¨åˆ†æˆåŠŸ" in message or "ä¸å…¨" in message:
                        retry_results['incomplete'] += 1
                        mark_retry_incomplete(slice_info, message)
                        logger.warning(f"[é‡è¯• {completed}/{len(time_slices)}] WARNING: {message}")
                    else:
                        retry_results['failed'] += 1
                        mark_retry_failed(slice_info, message.split('] ')[-1] if '] ' in message else message)
                        logger.error(f"[é‡è¯• {completed}/{len(time_slices)}] ERROR: {message}")
                    
            except Exception as e:
                retry_results['failed'] += 1
                slice_info = f"{slice_start}_to_{slice_end}"
                mark_retry_failed(slice_info, f"é‡è¯•Workerå¼‚å¸¸: {str(e)}")
                logger.error(f"[é‡è¯• {completed}/{len(time_slices)}] ERROR - Workerå¼‚å¸¸: {slice_start} åˆ° {slice_end}: {e}")
            
            # æ¯5ä¸ªä»»åŠ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ï¼ˆé‡è¯•æ—¶æ›´é¢‘ç¹æ˜¾ç¤ºï¼‰
            if completed % 5 == 0:
                logger.info(f"é‡è¯•è¿›åº¦: {completed}/{len(time_slices)} å®Œæˆ")
    
    # è¾“å‡ºé‡è¯•ç»“æœ
    if retry_results['failed_slices']:
        logger.info(f"ç¬¬ {retry_round} è½®é‡è¯•ä»ç„¶å¤±è´¥çš„åˆ‡ç‰‡ï¼š")
        for s in retry_results['failed_slices']:
            reason = retry_results.get('failed_reasons', {}).get(s, 'æœªçŸ¥åŸå› ')
            logger.info(f"- {s}: {reason}")

    if retry_results['incomplete_slices']:
        logger.info(f"ç¬¬ {retry_round} è½®é‡è¯•ä»ç„¶ä¸å…¨çš„åˆ‡ç‰‡ï¼š")
        for s in retry_results['incomplete_slices']:
            logger.info(f"- {s}")

    return retry_results

# --------------------------------------------------------------------------------------
# æ—¶é—´èŒƒå›´è·å–å‡½æ•°ï¼ˆæ”¯æŒ3ç§æ¨¡å¼ï¼‰
# --------------------------------------------------------------------------------------

def get_time_range_by_mode(mode: str, base_directory: Path, userkey: str, 
                          alpha_type: str = "regular") -> Tuple[str, str]:
    """
    æ ¹æ®æ¨¡å¼è·å–æ—¶é—´èŒƒå›´
    
    Args:
        mode: 'auto', 'incremental', 'manual'
        base_directory: æ•°æ®æ ¹ç›®å½•
        userkey: ç”¨æˆ·æ ‡è¯†
        alpha_type: 'regular' æˆ– 'super'
        
    Returns:
        (start_date_str, end_date_str): ISOæ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²
    """
    mode = mode.lower()
    
    if mode == "auto":
        # è‡ªåŠ¨æ¨¡å¼ï¼šåŸºäº DAYS é…ç½®
        days = int(_CONFIG["DAYS"])
        start_date, end_date = faa.get_dates_for_query(days)
        logger.info(f"[{alpha_type.upper()}] ä½¿ç”¨ auto æ¨¡å¼ï¼šè·å–è¿‡å» {days} å¤©çš„æ•°æ®")
        return start_date, end_date
        
    elif mode == "incremental":
        # å¢é‡æ¨¡å¼ï¼šä»æœ€æ–°æ–‡ä»¶çš„èµ·å§‹æ—¶é—´å¼€å§‹ï¼ˆæå‰ä¸€ä¸ªåˆ‡ç‰‡ï¼‰
        data_dir = base_directory / f"all_{alpha_type}_alphas"
        
        if not data_dir.exists():
            logger.warning(f"[{alpha_type.upper()}] å¢é‡æ¨¡å¼ï¼šç›®å½•ä¸å­˜åœ¨ {data_dir}ï¼Œå›é€€åˆ° auto æ¨¡å¼")
            days = int(_CONFIG["DAYS"])
            return faa.get_dates_for_query(days)
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
        import re
        pattern = re.compile(
            rf'{userkey}_(\d{{4}}-\d{{2}}-\d{{2}}-\d{{2}}-\d{{2}}-\d{{2}}\.\d{{3}}Z)_to_'
            rf'(\d{{4}}-\d{{2}}-\d{{2}}-\d{{2}}-\d{{2}}-\d{{2}}\.\d{{3}}Z)_all_{alpha_type}_alphas\.json'
        )
        
        files = []
        for f in data_dir.glob(f"{userkey}_*_all_{alpha_type}_alphas.json"):
            match = pattern.match(f.name)
            if match:
                # æ–‡ä»¶åæ ¼å¼: 2024-11-27-09-00-00.000Z
                # è½¬æ¢ä¸º: 2024-11-27T09:00:00.000Z
                file_time = match.group(1)
                # æ›¿æ¢å‰ä¸¤ä¸ª - åé¢çš„ - ä¸º :
                parts = file_time.split('-')
                if len(parts) >= 6:
                    start_str = f"{parts[0]}-{parts[1]}-{parts[2]}T{parts[3]}:{parts[4]}:{parts[5]}"
                else:
                    continue
                files.append((start_str, f))
        
        if not files:
            logger.warning(f"[{alpha_type.upper()}] å¢é‡æ¨¡å¼ï¼šæœªæ‰¾åˆ°å·²æœ‰æ–‡ä»¶ï¼Œå›é€€åˆ° auto æ¨¡å¼")
            days = int(_CONFIG["DAYS"])
            return faa.get_dates_for_query(days)
        
        # æ‰¾åˆ°æœ€æ–°æ–‡ä»¶çš„èµ·å§‹æ—¶é—´
        files.sort(key=lambda x: x[0], reverse=True)
        latest_start_time = files[0][0]
        
        # è·å–å½“å‰æ—¶é—´ï¼ˆå¯¹é½åˆ°æ•´ç‚¹ï¼‰
        now_utc = datetime.now(timezone.utc)
        end_date = now_utc.replace(minute=0, second=0, microsecond=0)
        end_date_str = end_date.strftime('%Y-%m-%dT%H:%M:%S.000Z')
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§å¹¶å‘å‡ºè­¦å‘Š
        total_files = len(files)
        oldest_start_time = files[-1][0] if files else None
        
        logger.info(f"[{alpha_type.upper()}] ä½¿ç”¨ incremental æ¨¡å¼ï¼šä» {latest_start_time} å¼€å§‹ï¼ˆè¦†ç›–æœ€åä¸€ä¸ªåˆ‡ç‰‡ï¼‰")
        logger.warning("=" * 80)
        logger.warning("WARNING: INCREMENTAL æ¨¡å¼è­¦å‘Š - è¯·ä»”ç»†é˜…è¯»")
        logger.warning("=" * 80)
        logger.warning(f"å½“å‰æ£€æµ‹åˆ° {total_files} ä¸ªå·²æœ‰æ–‡ä»¶")
        logger.warning(f"æœ€æ—©æ–‡ä»¶æ—¶é—´: {oldest_start_time}")
        logger.warning(f"æœ€æ–°æ–‡ä»¶æ—¶é—´: {latest_start_time}")
        logger.warning("")
        logger.warning("WARNING: é‡è¦æç¤ºï¼š")
        logger.warning("   1. incremental æ¨¡å¼åªä»ã€æœ€æ–°æ–‡ä»¶ã€‘å¼€å§‹è·å–æ–°æ•°æ®")
        logger.warning("   2. ä¸ä¼šæ£€æŸ¥ã€æœ€æ–°æ–‡ä»¶ä¹‹å‰ã€‘çš„å†å²æ•°æ®æ˜¯å¦å®Œæ•´")
        logger.warning("   3. å¦‚æœå†å²æ•°æ®ä¸­å­˜åœ¨ç¼ºå¤±çš„æ—¶é—´åˆ‡ç‰‡ï¼Œæœ¬æ¬¡è¿è¡Œä¸ä¼šè¡¥å……")
        logger.warning("")
        logger.warning("WARNING: æ½œåœ¨é£é™©ï¼š")
        logger.warning(f"   - ä» {oldest_start_time} åˆ° {latest_start_time} ä¹‹é—´å¯èƒ½å­˜åœ¨æ•°æ®ç¼ºå¤±")
        logger.warning("   - å¦‚æœä¹‹å‰çš„è¿è¡Œè¢«ä¸­æ–­ï¼Œå¯èƒ½æœ‰éƒ¨åˆ†æ—¶é—´åˆ‡ç‰‡æœªè·å–")
        logger.warning("   - å»ºè®®å®šæœŸä½¿ç”¨ auto æ¨¡å¼è¿›è¡Œå…¨é‡æ£€æŸ¥")
        logger.warning("")
        logger.warning("NOTE: å¦‚éœ€æ£€æŸ¥å¹¶è¡¥å……å†å²ç¼ºå¤±æ•°æ®ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ¨¡å¼ä¹‹ä¸€ï¼š")
        logger.warning("   - auto æ¨¡å¼ + OVERWRITE=falseï¼ˆè¡¥å……æ‰€æœ‰ç¼ºå¤±æ–‡ä»¶ï¼‰")
        logger.warning("   - manual æ¨¡å¼æŒ‡å®šæ—¶é—´èŒƒå›´ + OVERWRITE=falseï¼ˆè¡¥å……ç‰¹å®šæ—¶æ®µï¼‰")

        
        return latest_start_time, end_date_str
        
    elif mode == "manual":
        # æ‰‹åŠ¨æ¨¡å¼ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ START_TIME å’Œ END_TIME
        start_time = _CONFIG.get("START_TIME")
        end_time = _CONFIG.get("END_TIME")
        
        if not start_time or not end_time:
            raise ValueError(
                "manual æ¨¡å¼éœ€è¦åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® START_TIME å’Œ END_TIME\n"
                "æ ¼å¼ç¤ºä¾‹: \"2024-11-01T00:00:00.000Z\""
            )
        
        logger.info(f"[{alpha_type.upper()}] ä½¿ç”¨ manual æ¨¡å¼ï¼š{start_time} åˆ° {end_time}")
        return start_time, end_time
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å¼: {mode}ï¼Œæ”¯æŒçš„æ¨¡å¼: auto, incremental, manual")


# --------------------------------------------------------------------------------------
# ä¸»å‡½æ•°
# --------------------------------------------------------------------------------------

def main():
    # ä»é…ç½®è·å–è¿è¡ŒæœŸå‚æ•°ï¼ˆè‹¥æ—  async_config.json åˆ™ä½¿ç”¨ DEFAULT_CONFIGï¼‰
    fetch_mode = _CONFIG.get("FETCH_MODE", "auto").lower()  # è·å–æ¨¡å¼
    overwrite = bool(_CONFIG["OVERWRITE"])     # æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
    skip_regular = bool(_CONFIG["SKIP_REGULAR"])   # æ˜¯å¦è·³è¿‡ Regular Alphas
    skip_super = bool(_CONFIG["SKIP_SUPER"])       # æ˜¯å¦è·³è¿‡ Super Alphas
    max_workers = int(_CONFIG["MAX_WORKERS"])  # å¹¶å‘ worker æ•°é‡
    interval_hours = int(_CONFIG["INTERVAL_HOURS"])  # æ—¶é—´åˆ‡ç‰‡é—´éš”ï¼ˆå°æ—¶ï¼‰
    base_directory_override = _CONFIG.get("BASE_DIRECTORY")  # å¯é€‰ï¼šè¦†ç›–æ•°æ®è¾“å‡ºæ ¹ç›®å½•
    retry_rounds = int(_CONFIG.get("RETRY_ROUNDS", 3))  # å¤±è´¥åˆ‡ç‰‡é‡è¯•è½®æ•°
    retry_workers = int(_CONFIG.get("RETRY_WORKERS", 2))  # é‡è¯•æ—¶ä½¿ç”¨çš„workeræ•°é‡

    try:
        # åˆå§‹åŒ–ä¼šè¯ç®¡ç†å™¨
        session_manager = SessionManager()
        
        # åˆ›å»ºSessionProxyï¼Œç¡®ä¿æ¯æ¬¡è¯·æ±‚éƒ½ä½¿ç”¨æœ€æ–°session
        session_proxy = SessionProxy(session_manager)
        
        # æå‰åˆå§‹åŒ–ä¼šè¯ï¼Œé¿å…åœ¨ worker çº¿ç¨‹ä¸­åˆå§‹åŒ–å¯¼è‡´æ—¥å¿—æ··ä¹±
        logger.info("æ­£åœ¨åˆå§‹åŒ–ä¼šè¯...")
        _ = session_manager.get_session()  # è§¦å‘ä¼šè¯åˆå§‹åŒ–
        logger.info("ä¼šè¯åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨SessionProxyç¡®ä¿å§‹ç»ˆè·å–æœ€æ–°sessionï¼‰")

        userkey = wq_login.USER_KEY
        base_directory = faa.base_directory
        # è¦†ç›–æ•°æ®è¾“å‡ºç›®å½•ï¼ˆå¦‚æä¾›ï¼‰
        if base_directory_override:
            base_directory = Path(base_directory_override)
            base_directory.mkdir(parents=True, exist_ok=True)
            logger.info(f"ä½¿ç”¨é…ç½®çš„è¦†ç›–æ•°æ®è¾“å‡ºç›®å½•: {base_directory}")

        # æ‰€æœ‰æ—¥å¿—ç»Ÿä¸€é€šè¿‡ wq_logger.py å¤„ç†ï¼Œä¸å†åˆ›å»ºç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶

        logger.info("=" * 80)
        logger.info("WorldQuant Brain Alphaæ•°æ®è·å–å™¨ - å¼‚æ­¥ç‰ˆæœ¬")
        logger.info("=" * 80)
        logger.info(f"è·å–æ¨¡å¼: {fetch_mode.upper()}")
        logger.info(f"ç”¨æˆ·æ ‡è¯†: {userkey}")
        logger.info(f"åŸºç¡€ç›®å½•: {base_directory}")
        logger.info(f"è¦†ç›–æ¨¡å¼: {'æ˜¯' if overwrite else 'å¦'}")
        logger.info(f"å¹¶å‘Workeræ•°: {max_workers}")
        logger.info(f"é‡è¯•é…ç½®: {retry_rounds} è½®é‡è¯•ï¼Œæ¯è½® {retry_workers} ä¸ªworker")
        logger.info("=" * 80)

        total_start_time = time.time()

        # è·å– Regular Alphas
        if not skip_regular:
            logger.info("\nå¼€å§‹å¼‚æ­¥è·å– Regular Alphas")
            logger.info("=" * 50)

            # æ ¹æ®æ¨¡å¼è·å–æ—¶é—´èŒƒå›´
            start_date_input, end_date_input = get_time_range_by_mode(
                fetch_mode, base_directory, userkey, "regular"
            )
            logger.info(f"æ—¶é—´èŒƒå›´: {start_date_input} åˆ° {end_date_input}")

            start_time = time.time()
            regular_results = fetch_alpha_async(
                session_proxy, session_manager, userkey, start_date_input, end_date_input,
                base_directory, "regular", overwrite, max_workers, interval_hours
            )
            end_time = time.time()
            duration = end_time - start_time

            logger.info(f"\nRegular Alphas å¤„ç†å®Œæˆ (è€—æ—¶: {duration:.1f}ç§’):")
            logger.info(f"  æ€»è®¡: {regular_results['total']} ä¸ªæ—¶é—´åˆ‡ç‰‡")
            logger.info(f"  å·²å¤„ç†: {regular_results['processed']} ä¸ª")
            logger.info(f"  å·²è·³è¿‡: {regular_results['skipped']} ä¸ª")
            logger.info(f"  å¤±è´¥: {regular_results['failed']} ä¸ª")
            logger.info(f"  ä¸å…¨: {regular_results['incomplete']} ä¸ª")

        # è·å– Super Alphas
        if not skip_super:
            logger.info("\nå¼€å§‹å¼‚æ­¥è·å– Super Alphas")
            logger.info("=" * 50)

            # æ ¹æ®æ¨¡å¼è·å–æ—¶é—´èŒƒå›´
            start_date_input, end_date_input = get_time_range_by_mode(
                fetch_mode, base_directory, userkey, "super"
            )
            logger.info(f"æ—¶é—´èŒƒå›´: {start_date_input} åˆ° {end_date_input}")

            start_time = time.time()
            super_results = fetch_alpha_async(
                session_proxy, session_manager, userkey, start_date_input, end_date_input,
                base_directory, "super", overwrite, max_workers, interval_hours
            )
            end_time = time.time()
            duration = end_time - start_time

            logger.info(f"\nSuper Alphas å¤„ç†å®Œæˆ (è€—æ—¶: {duration:.1f}ç§’):")
            logger.info(f"  æ€»è®¡: {super_results['total']} ä¸ªæ—¶é—´åˆ‡ç‰‡")
            logger.info(f"  å·²å¤„ç†: {super_results['processed']} ä¸ª")
            logger.info(f"  å·²è·³è¿‡: {super_results['skipped']} ä¸ª")
            logger.info(f"  å¤±è´¥: {super_results['failed']} ä¸ª")
            logger.info(f"  ä¸å…¨: {super_results['incomplete']} ä¸ª")

        total_end_time = time.time()
        total_duration = total_end_time - total_start_time

        logger.info(f"\nSUCCESS: æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ (æ€»è€—æ—¶: {total_duration:.1f}ç§’)")

        # æ˜¾ç¤ºæœ€ç»ˆä¼šè¯çŠ¶æ€
        remaining_time = session_manager.get_remaining_time()
        logger.info(f"ä¼šè¯å‰©ä½™æ—¶é—´: {int(remaining_time/60)} åˆ†é’Ÿ")

    except KeyboardInterrupt:
        logger.warning("\nWARNING: ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise

# --------------------------------------------------------------------------------------
# å…¥å£
# --------------------------------------------------------------------------------------

if __name__ == "__main__":
    main()